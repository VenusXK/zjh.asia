# Hadoop实验扩展部分(9节点)

[[TOC]]

## 实例配置

### 新建实例

在AWS上新建另外 7 个实例，配置如下：

- **实例的数量**：7
- **Software Image (AMI)**：Canonical, Ubuntu, 22.04 LTS, ... ami-0fc5d935ebf8bc3bc
- **Virtual server type (instance type)**：t2.medium
- **Firewall (security group)**：launch-wizard-1
- **Storage (volumes)**：1 volume(s) - 8 GiB

建立过程中，发现第9个实例无法建立，被系统终止了，提供信息如下：

```
停止 – 休眠操作
已禁用

状态转换原因
User initiated (2023-12-14 14:34:35 GMT)

状态转换消息
 Client.UserInitiatedShutdown: User initiated shutdown
```

经过查阅，得知如下消息：

> 当达到弹性 block 存储 (EBS) 卷限制时，尤其是在免费服务器上时，就会发生这种情况。

重新建立实例，留意到EBS模块有如下信息：


> Free tier eligible customers can get up to 30 GB of EBS General Purpose (SSD) or Magnetic storage
> 
> 所选 AMI 包含的实例存储卷数超过了实例允许的数量。只能从该实例访问该 AMI 的前 0 个实例存储卷

AMI 指的是 Amazon Machine Image，意思指 Ubuntu 实例搭载的 EBS 过多了，因此终止 `slave-node-1` 和 `slave-node-2` 两个实例，因为其存储为 15 GB，计划分配 8 GB，腾出近 14 GB，分配给新的实例重新建立，选择 `General Purpose (SSD)`，并且选择 `Delete on Termination`，这样就可以避免 EBS 过多的问题。

尝试后发现依然无法解决该问题，第10个节点依然无法建立，重新阅读老师发的实验指导，发现下面这句话：

> Maximum of 9 concurrently running EC2 instances, regardless of instance size. If you attempt to launch more, the excess instances will be terminated (and 9 will be left running).

因此理解为学习系统的限制，只要超过9个结点必被终止，因此将题目要求的"将集群数量扩大至9个节点"理解为总共9个节点，只有8个从节点，继续进行配置。

### 创建新用户

根据上一章的指导，建立新用户 `hadoop` ，并且将其加入 `sudo` 组。


### 配置节点连接

执行 `sudo vim /etc/hostname` 修改各个节点的主机名，具体参照上一章。

执行 `sudo vim /etc/hosts` 修改自己所用节点的IP映射，注意这里的 IP 地址是上文所述的私有 IP。

```shell
172.31.91.103 master
172.31.84.23 slave1
172.31.90.72 slave2
172.31.94.13 slave3
172.31.85.67 slave4
172.31.86.119 slave5
172.31.82.175 slave6
172.31.89.32 slave7
172.31.86.65 slave8
```

## 更新 master 节点

通过 ssh 登录到 master 节点，如下：

```shell
ssh -i zjh-hadoop.pem ubuntu@ec2-52-87-240-67.compute-1.amazonaws.com
```

按照上一小节修改其 hosts 文件，之后尝试 ping 操作，如下：

```shell
hadoop@master:/home/ubuntu$ ping slave1
hadoop@master:/home/ubuntu$ ping slave2
... ...
```

可以完成 ping 操作，说明各个节点之间可以互相访问。


## 配置 Java 环境

直接将 Java 安装程序压缩包通过 scp 传输到各个节点，解压后将其移动到 `/usr/lib/` 目录下，即执行如下命令：

```shell
scp ~/jdk-8u231-linux-x64.tar.gz slave1:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave2:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave3:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave4:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave5:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave6:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave7:/home/hadoop
scp ~/jdk-8u231-linux-x64.tar.gz slave8:/home/hadoop
```

### 解决 hosts 目标主机变更后的不匹配问题

```shell
hadoop@master:/home/ubuntu$ scp ~/jdk-8u231-linux-x64.tar.gz slave1:/home/hadoop
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ED25519 key sent by the remote host is
SHA256:DpE4DpsLmROmy3w1IXeP43axBpTJStCa7iVt/Ki9rbI.
Please contact your system administrator.
Add correct host key in /home/hadoop/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /home/hadoop/.ssh/known_hosts:6
  remove with:
  ssh-keygen -f "/home/hadoop/.ssh/known_hosts" -R "slave1"
Host key for slave1 has changed and you have requested strict checking.
Host key verification failed.
lost connection
```

问题产生原因如下：我一开始建立了slave1和slave2，并通过ssh记录到master的known_hosts文件中，现在换掉了slave1的IP，因此ssh认为slave1主机和之前记录的ip不同，因此报错，因此执行如下语句：

```shell
ssh-keygen -f "/home/hadoop/.ssh/known_hosts" -R "slave1"
```

解释：`-f filename` 指定密钥文件名，`-R hostname` 删除指定主机名的密钥。

执行后返回如下内容：

```shell
hadoop@master:/home/ubuntu$ ssh-keygen -f "/home/hadoop/.ssh/known_hosts" -R "slave1"
# Host slave1 found: line 2
# Host slave1 found: line 5
# Host slave1 found: line 6
/home/hadoop/.ssh/known_hosts updated.
Original contents retained as /home/hadoop/.ssh/known_hosts.old
```

重新执行 `scp ~/jdk-8u231-linux-x64.tar.gz slave1:/home/hadoop`，显示正常。

### slave 安装 Java

按照上一节的方法，安装 Java，并修改环境变量，具体参照上一章。

## 部署并运行 Hadoop

将 master 上的 `/usr/lib/hadoop` 文件夹复制到各个 slave 节点上，具体参照上一章，传输时执行如下命令：

```shell
scp ~/hadoop.master.tar.gz slave1:/home/hadoop
scp ~/hadoop.master.tar.gz slave2:/home/hadoop
scp ~/hadoop.master.tar.gz slave3:/home/hadoop
scp ~/hadoop.master.tar.gz slave4:/home/hadoop
scp ~/hadoop.master.tar.gz slave5:/home/hadoop
scp ~/hadoop.master.tar.gz slave6:/home/hadoop
scp ~/hadoop.master.tar.gz slave7:/home/hadoop
scp ~/hadoop.master.tar.gz slave8:/home/hadoop
```

在两个 slave 节点上执行如下命令：

```shell
sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/lib
sudo chown -R hadoop /usr/lib/hadoop
```

### 修改配置文件

master 修改 `/usr/lib/hadoop/etc/hadoop/slaves`

文件 slaves 把里面的 localhost 改为如下内容：

```shell
slave1
slave2
slave3
slave4
slave5
slave6
slave7
slave8
```

### 初始化 HDFS

在 master 节点上执行如下命令：

```shell
hdfs namenode -format
```

### 使 master 免密登录到各节点

将 master 的 ssh 密钥复制到各个 slave 节点上，在执行 `start-dfs.sh` 和 `start-yarn.sh` 时可以免密登录。

```shell
# 将密钥传到 slave 节点
scp ~/.ssh/id_rsa.pub slave1:/home/hadoop/ 
scp ~/.ssh/id_rsa.pub slave2:/home/hadoop/
scp ~/.ssh/id_rsa.pub slave3:/home/hadoop/
scp ~/.ssh/id_rsa.pub slave4:/home/hadoop/
scp ~/.ssh/id_rsa.pub slave5:/home/hadoop/
scp ~/.ssh/id_rsa.pub slave6:/home/hadoop/
scp ~/.ssh/id_rsa.pub slave7:/home/hadoop/
scp ~/.ssh/id_rsa.pub slave8:/home/hadoop/
```

接着在 slave1 和 slave2 节点上，将 ssh 公匙加入授权：

```shell
mkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略
cat ~/id_rsa.pub >> ~/.ssh/authorized_keys
```

### 启动 hadoop


按照上一章的方法，启动 hadoop。

```shell
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver
```

返回如下信息：

```shell
hadoop@master:/usr/lib$ start-dfs.sh

Starting namenodes on [Master]
Master: namenode running as process 1120. Stop it first.
slave1: datanode running as process 1729. Stop it first.
slave2: datanode running as process 1905. Stop it first.
slave7: datanode running as process 2131. Stop it first.
slave5: datanode running as process 2062. Stop it first.
slave3: datanode running as process 2292. Stop it first.
slave4: datanode running as process 2724. Stop it first.
slave8: datanode running as process 1685. Stop it first.
slave6: datanode running as process 1937. Stop it first.
Starting secondary namenodes [Master]
Master: secondarynamenode running as process 1336. Stop it first.

hadoop@master:/usr/lib$ start-yarn.sh

starting yarn daemons
starting resourcemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-resourcemanager-master.out
slave7: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave7.out
slave6: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave6.out
slave5: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave5.out
slave8: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave8.out
slave4: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave4.out
slave3: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave3.out
slave1: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave1.out
slave2: starting nodemanager, logging to /usr/lib/hadoop/logs/yarn-hadoop-nodemanager-slave2.out

hadoop@master:/usr/lib$ mr-jobhistory-daemon.sh start historyserver

starting historyserver, logging to /usr/lib/hadoop/logs/mapred-hadoop-historyserver-master.out

hadoop@master:/usr/lib$ jps

1120 NameNode
3747 JobHistoryServer
3462 ResourceManager
1336 SecondaryNameNode
3784 Jps
```

说明搭建完成，master和各节点正常运行。

### 访问 webUI

### 运行 `hadoop-mapreduce-examples-*.jar`

```shell
hadoop fs -mkdir -p /user/hadoop   # 在hdfs上创建hadoop账户
hadoop fs -mkdir input
hadoop fs -put /usr/lib/hadoop/etc/hadoop/*.xml input  # 将hadoop配置文件复制到hdfs中
hadoop jar /usr/lib/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+'  # 运行实例
```
